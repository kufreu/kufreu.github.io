---
output : 'github_document'
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = T, fig.retina = 5, warning = F, message = F)
```

# zonal statistics and land cover in chicago
Using a [high-resolution land cover raster](https://datahub.cmap.illinois.gov/dataset/high-resolution-land-cover-cook-county-2010) of Cook County obtained from the data hub of the Chicago Metropolitan Agency for Planning (CMAP), the goal for this section is to determine how much tree canopy each tract in Chicago has with zonal statistics. There are many ways to go about calculating zonal statistics, such as using packages in R like `exactexractr` and `stars`, though here I have have decided on using `rasterstats` and other packages in Python to do so because it is rather straightforward. The first step was to import the necessary libraries for this analysis.

```{python import packages}
import pandas as pd
import geopandas as gpd
import rasterio as rio
from matplotlib import pyplot as plt
from rasterstats import zonal_stats
```

The next and somewhat important step is to find data to spatially aggregate the land cover by. For this I have chosen to use [census tracts boudaries](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Census-Tracts-2010/5jrd-6zik) in Chicago obtained from the City's data portal. In the code chunk below, the census tracts are read and reprojected to the coordinate reference system of the land cover raster.

```{python getting chicago}
raster = 'cookcountylandcover2010\landcover_2010_cookcounty.img'

with rio.open(raster) as src:
    chicago = gpd.read_file('https://data.cityofchicago.org/api/geospatial/5jrd-6zik?method=export&format=GeoJSON').to_crs(src.crs)
```
`zonal_stats` from `rasterstats` can not only treat rasters as categorical, which is important in this as my only concern is the pixel count of each land cover class, but it has a parameter to map categorical names to the pixel values with a dictionary. I can use the attribute table from the raster to take advantage of this.
```{python dbf file}
dbf = gpd.read_file('cookcountylandcover2010\landcover_2010_cookcounty.img.vat.dbf')

dbf
```
I can see here how the values map to the different land cover classes. What will need to be done is to clean up the land cover classes for use as column names and to create a dictionary with the pixel values as keys and the classes as values. 


```{python creating a dictionary for zonal stats}

classes = [str(x).lower().replace('/','_').replace(' ','_') for x in dbf['Class']]

land_cover = dict(zip(dbf['Value'].tolist(), classes))

```
Now that I have the dictionary with value-class pairs, I can use `zonal_stats`.
```{python zonal}
zonal = zonal_stats(chicago,raster,categorical = True, nodata = 0, category_map = land_cover)

zonal[0]
```

For each census tract, a dictionary is created with a pixel count of the land cover class found within the tract.  Before doing this, however, I needed to create a somewhat empty `DataFrame` which to put the values from `zonal_stats` into. The categorical names from `land_cover` were  used as column names in this `DataFrame` and `geoid10` was added to identify each tract and to allow for joins, though the index of `DataFrame` could also be used for the latter.

```{python creating a dataframe}
zeros = [0] * len(chicago)

results = pd.DataFrame(dict((v,zeros) for k,v in land_cover.items()))

results.insert(0,'geoid10', chicago['geoid10'])

results
```

With the `DataFrame` created, what is next is to input the data using a loop. There is most definitely a better way of doing this, both using the results from `zonal_stats` or creating a `DataFrame` from a list of dictionaries, though this way was still trouble-free.  The values are multiplied by four in the loop because the spatial resolution of the raster is 2 survey feet. 

```{python adding values}
for i in range(len(zonal)):
    row = zonal[i]
    for j in list(row.keys()):
        results.at[i,j] = row.get(j) * 4

results
```

The results can now be joined to the tracts and  the tree coverage of each tract can be determined by dividing `tree_canopy` by the tract area.

```{python canopy}
chicago = chicago.merge(results, how = 'left', on = 'geoid10')

chicago['pct_canopy'] = chicago['tree_canopy'] / chicago.area * 100
```

I can take a quick look at the tracts to see if `pct_canopy` appears to be reasonable. 

```{python canopy map}
fig, ax = plt.subplots(1,1)
ax.axes.xaxis.set_visible(False)
ax.axes.yaxis.set_visible(False)

chicago.plot(column='pct_canopy', 
             legend=True,
             ax = ax,
             legend_kwds={'label': "Canopy Cover"}
             )
```

Although a pattern can be seen in the distribution of tree canopy in the city, such as there being less tree coverage in and around the loop, this map does not go as far as showing if and how this distribution maps to race in Chicago. The final step is to save the results and to continue on with the analysis in R by using `tidycensus` to collect demographic data for the city. 

```{python save results}
results.to_csv('chicago_land_cover.csv')
chicago.to_file('chicago.gpkg', layer ='land_cover', driver="GPKG")
```